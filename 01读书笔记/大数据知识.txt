大数据知识积累
大数据是人们获得新的认知、创造新的价值的源泉；大数据还是改变市场、组织机构，以及政府与公民关系的方法。
大数据，是量变导致质变的过程。
	我们就以纳米技术为例。纳米技术专注于把东西变小而不是变大。其原理就是当事务达到分子的级别是，它的物理性质就会发生改变。
	一旦你知道这些新的性质，你就可以用同样的原料来做以前无法做的事情。铜本来是用力啊到点的物质，但它一旦到达纳米级别就不能在磁场中导电了。
大数据的核心就是预测，把数学算法运用到海量的数据上来预测事情发生的可能性。	

大数据与三个重大的思维转变有关，这三个转变是相互联系和相互作用的。
	首先，分析与某事物相关的所有数据，而不是依靠分析少量的数据样本。
	其次，要乐于接受数据的纷繁复杂，而不再追求数据的精确性。
	最后，重点不再去探求难以捉摸的因果关系，转而关注事物的相关关系。

采样分析的精确性随着采样随机性的增加而大幅提高，但这与数量增加关系不大。
	比如，随机调查1100人得出20个人都19个人都能答正确。不管是调查10万人还是1亿人，得出20个人都19个人都能答对这个结论几乎很少变化了
	为什么会这样呢？原因很复杂，但是有一个比较简单的解释就是，当样本数量达到了某个值之后，我们从新个体身上得到的信息就会越来越少，就如同经济学中的编辑效应递减一样。

随机采样取得了巨大的成功，成为现代社会、现代测量领域的主心骨。但这只是一条捷径，是在不可手机和分析全部数据的情况下的选择，它本省存在许多固有的缺陷。他的成功依赖于采样的绝对随机性，但是实现采样的随机性非常困难。一旦采样过程中存在任何偏见，分析结果就会想去甚远。
	p{ 每一个人都不可能做到绝对的公平。比如不同的人对同一件事务都有不同的看法，有可能觉得路程太远而抛弃了某一片区的采样。也有可能当地政府不配合，导致采样数据毫无头绪。
	   或者是设计抽样调查方案就由于人为考虑不周全而忽视了一大块领域。
	}
	
论述在大数据时代，全数据模式，样本=总体的不实际性
	采样的目的就是用最少的数据得到最多的信息。当我们可以获得海量数据的时候，它就没有什么意义了。数据处理技术已经发生了翻天覆地的改变，但我们的方法和思维却没有跟上这种改变。
	
	采样一直有一个被我们广泛承认却又总有意避开的缺陷，现在这个缺陷越来越难以忽视了。采样忽视了细节的考察。
	虽然我们别无选择，只能利用采样分析来进行考察，但是在很多领域，从收集部分数据到收集尽可能多的数据的转变已经发生了。
	如果可能的的话，我们会搜集所有的数据，即“样本=总体”。
	
	正如我们所看到的，“样本=总体”是指我们能对数据进行深度探讨，而采样几乎无法达到这样的效果。
	上面提到的有关采样的例子证明，用采样的方法分析整个人口的情况，正确率可达97%。对于某些事物来说，3%的错误是可以接受的。
	但是你无法得到一些围观细节的信息，甚至还会失去对某些特定子类别进行进一步研究的能力。
	我们不能瞒住与正态分布一般中庸平凡的景象。生活中真正有趣的事情经常藏匿在细节之中，而采样分析法却无法捕捉到这些细节。
	
大数据是指不用水机分析法这样的捷径，而采用所有数据的方法。
===============
02更杂：不是精确新，而是混杂性
大数据洞察
	“大数据”通常用概率说话，而不是板着“确凿无疑”的面孔。整个社会要习惯这种思维需要很长的时间，其中也会出现一些问题。但现在，有必要指出的是，当我们试图扩大数据规模的时候，要学会拥抱混乱。
	
	
	
